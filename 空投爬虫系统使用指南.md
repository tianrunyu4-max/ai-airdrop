# 🚀 空投爬虫系统使用指南

## 📋 目录
1. [系统架构](#系统架构)
2. [快速开始](#快速开始)
3. [数据库配置](#数据库配置)
4. [爬虫部署](#爬虫部署)
5. [数据源说明](#数据源说明)
6. [推送策略](#推送策略)
7. [管理和维护](#管理和维护)

---

## 🏗️ 系统架构

```
┌──────────────────┐     ┌──────────────────┐     ┌──────────────────┐
│  爬虫服务器       │────▶│  Supabase DB     │◀────│  聊天机器人       │
│  (Node.js)       │     │  airdrops表      │     │  (Vue.js)        │
│                  │     │                  │     │                  │
│  - Layer3        │     │  - 90% Web3空投  │     │  - 每天10:00推送  │
│  - Galxe         │     │  - 10% CEX空投   │     │  - 每天20:00推送  │
│  - DeFiLlama     │     │  - AI智能评分    │     │  - 5-10个/次     │
│  - Twitter       │     │  - 自动去重      │     │                  │
│  - 中文资讯      │     │                  │     │                  │
└──────────────────┘     └──────────────────┘     └──────────────────┘
     每6小时执行              数据存储中心              实时推送到群聊
```

---

## ⚡ 快速开始

### 第1步：部署数据库

在 Supabase SQL Editor 中执行：

```sql
-- 创建数据库表
\i supabase/migrations/创建空投爬虫系统.sql
```

### 第2步：安装依赖

```bash
npm install axios cheerio node-cron
```

### 第3步：配置环境变量

创建 `.env` 文件：

```bash
# Supabase
VITE_SUPABASE_URL=你的Supabase项目URL
VITE_SUPABASE_ANON_KEY=你的Supabase匿名Key

# Twitter API（可选）
TWITTER_BEARER_TOKEN=你的Twitter API Token
```

### 第4步：启动爬虫

```bash
# 测试运行（立即执行一次）
node scripts/airdrop-crawler.mjs

# 后台运行（定时任务）
nohup node scripts/airdrop-crawler.mjs > crawler.log 2>&1 &

# 使用PM2管理（推荐）
npm install -g pm2
pm2 start scripts/airdrop-crawler.mjs --name airdrop-crawler
pm2 logs airdrop-crawler
pm2 status
```

---

## 🗄️ 数据库配置

### 核心表结构

#### 1. `airdrops` 表（空投信息）

| 字段 | 类型 | 说明 |
|------|------|------|
| `id` | UUID | 主键 |
| `title` | TEXT | 空投标题 |
| `platform` | TEXT | 来源平台（Layer3/Galxe等） |
| `project_name` | TEXT | 项目名称 |
| `type` | TEXT | 类型（web3/cex） |
| `ai_score` | DECIMAL | AI评分（0-10） |
| `reward_min` | INTEGER | 最低奖励（USDT） |
| `reward_max` | INTEGER | 最高奖励（USDT） |
| `difficulty` | TEXT | 难度（easy/medium/hard） |
| `status` | TEXT | 状态（active/expired） |
| `push_count` | INTEGER | 已推送次数 |
| `last_pushed_at` | TIMESTAMP | 最后推送时间 |

#### 2. `airdrop_pushes` 表（推送历史）

记录每次推送的历史，用于统计和分析。

#### 3. `airdrop_sources` 表（数据源配置）

管理各个爬虫数据源的配置和状态。

---

## 🕷️ 爬虫部署

### 方案A：本地服务器（推荐）

**优点**：完全控制，无限制  
**缺点**：需要24小时运行的服务器

```bash
# 1. 克隆代码到服务器
git clone <your-repo-url>
cd AI智能空投

# 2. 安装依赖
npm install

# 3. 配置环境变量
cp .env.example .env
nano .env

# 4. 使用PM2启动
pm2 start scripts/airdrop-crawler.mjs --name airdrop-crawler
pm2 save
pm2 startup
```

### 方案B：云函数（Vercel/Netlify）

**优点**：免费，无需服务器  
**缺点**：有执行时间限制（10秒）

```javascript
// api/crawler.js (Vercel Serverless Function)
import { crawlAllSources } from '../scripts/airdrop-crawler.mjs'

export default async function handler(req, res) {
  if (req.method !== 'POST' || req.headers['authorization'] !== 'Bearer YOUR_SECRET') {
    return res.status(403).json({ error: 'Forbidden' })
  }
  
  const results = await crawlAllSources()
  res.json({ success: true, data: results })
}
```

然后使用 GitHub Actions 定时触发：

```yaml
# .github/workflows/crawler.yml
name: Run Airdrop Crawler

on:
  schedule:
    - cron: '0 */6 * * *'  # 每6小时执行

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Trigger Crawler
        run: |
          curl -X POST https://your-domain.vercel.app/api/crawler \
            -H "Authorization: Bearer ${{ secrets.CRAWLER_SECRET }}"
```

### 方案C：Docker容器

```dockerfile
# Dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install --production
COPY . .
CMD ["node", "scripts/airdrop-crawler.mjs"]
```

```bash
# 构建并运行
docker build -t airdrop-crawler .
docker run -d --name crawler airdrop-crawler
```

---

## 📊 数据源说明

### 1. Layer3.xyz（⭐⭐⭐⭐⭐）

**特点**：任务型空投，奖励明确  
**数据质量**：高  
**爬取难度**：中（需要API Key或网页爬虫）

**示例**：
```javascript
// API调用
const response = await axios.get('https://layer3.xyz/api/quests')
```

### 2. Galxe.com（⭐⭐⭐⭐⭐）

**特点**：最大的Web3任务平台  
**数据质量**：高  
**爬取难度**：中（GraphQL API）

**示例**：
```javascript
// GraphQL查询
const query = `
  query CampaignList {
    campaigns(first: 20, orderBy: "trending") {
      list { id, name, description }
    }
  }
`
```

### 3. DeFiLlama（⭐⭐⭐⭐）

**特点**：DeFi数据聚合  
**数据质量**：中（需要人工判断）  
**爬取难度**：低（公开API）

**策略**：筛选无代币但TVL高的项目 = 潜在空投

### 4. Twitter（⭐⭐⭐）

**特点**：实时性强  
**数据质量**：低（噪音多）  
**爬取难度**：高（需要API Key，且有费用）

**建议**：仅作为辅助数据源

### 5. 中文资讯（⭐⭐⭐⭐）

**平台**：Foresight News, PANews, 链捕手  
**特点**：中文友好，整理好的信息  
**爬取难度**：中（网页爬虫）

---

## 🎯 推送策略

### 推送时间

- **早上 10:00**：第1批（5-10个空投）
- **晚上 20:00**：第2批（5-10个空投）

### 选择逻辑

1. **评分筛选**：只推送 AI评分 ≥ 7.0 的优质空投
2. **类型分配**：90% Web3 + 10% CEX
3. **去重机制**：12小时内推送过的不重复推送
4. **优先级排序**：
   - AI评分从高到低
   - priority字段从高到低
   - 创建时间从新到旧

### AI评分算法

```javascript
function calculateAIScore(airdrop) {
  let score = 5.0
  
  // 奖励金额（最高+3分）
  if (airdrop.reward_max > 5000) score += 3.0
  else if (airdrop.reward_max > 1000) score += 2.0
  else if (airdrop.reward_max > 500) score += 1.0
  
  // 难度（越简单分数越高，最高+2分）
  if (airdrop.difficulty === 'easy') score += 2.0
  else if (airdrop.difficulty === 'medium') score += 1.0
  
  // 平台可信度（最高+1.5分）
  if (['Layer3', 'Galxe', 'DeFiLlama'].includes(airdrop.platform)) {
    score += 1.5
  }
  
  // 限制在0-10之间
  return Math.min(Math.max(score, 0), 10)
}
```

---

## 🔧 管理和维护

### 1. 查看爬虫运行状态

```bash
# PM2管理
pm2 status
pm2 logs airdrop-crawler --lines 100

# Docker管理
docker logs -f crawler
```

### 2. 手动触发爬虫

```bash
# 直接执行
node scripts/airdrop-crawler.mjs

# 或通过PM2重启
pm2 restart airdrop-crawler
```

### 3. 查看数据库数据

```sql
-- 查看空投总数
SELECT COUNT(*) FROM airdrops;

-- 查看活跃空投
SELECT * FROM airdrops WHERE status = 'active' ORDER BY ai_score DESC LIMIT 10;

-- 查看推送历史
SELECT 
  a.title,
  a.ai_score,
  a.push_count,
  a.last_pushed_at
FROM airdrops a
ORDER BY push_count DESC
LIMIT 20;

-- 查看各平台数据量
SELECT 
  platform,
  COUNT(*) as count,
  AVG(ai_score) as avg_score
FROM airdrops
GROUP BY platform
ORDER BY count DESC;
```

### 4. 清理过期空投

```sql
-- 标记过期空投
UPDATE airdrops
SET status = 'expired'
WHERE deadline < NOW()
  AND status = 'active';

-- 删除30天前的过期空投
DELETE FROM airdrops
WHERE status = 'expired'
  AND created_at < NOW() - INTERVAL '30 days';
```

### 5. 人工审核和优化

```sql
-- 标记为已审核
UPDATE airdrops
SET is_verified = true, priority = 90
WHERE id = 'xxx';

-- 调整评分
UPDATE airdrops
SET ai_score = 9.5
WHERE id = 'xxx';

-- 禁用某个空投
UPDATE airdrops
SET status = 'inactive'
WHERE id = 'xxx';
```

---

## 📈 性能优化

### 1. 爬虫优化

- 使用并行请求（`Promise.all`）
- 添加请求重试机制
- 设置合理的超时时间
- 使用代理池（避免IP封禁）

### 2. 数据库优化

- 定期清理过期数据
- 添加必要的索引
- 使用视图简化查询

### 3. 推送优化

- 使用缓存减少数据库查询
- 批量推送而非单条推送
- 异步处理推送结果

---

## 🔒 安全建议

1. **API Key保护**：所有API Key存储在环境变量中
2. **权限控制**：数据库RLS策略限制访问
3. **爬虫限速**：避免过于频繁的请求
4. **数据验证**：存入数据库前验证数据格式

---

## 🐛 故障排查

### 问题1：爬虫无法启动

**原因**：依赖未安装或环境变量未配置  
**解决**：
```bash
npm install
cp .env.example .env
nano .env
```

### 问题2：爬取失败

**原因**：网站结构变化或API失效  
**解决**：
- 检查网站是否正常访问
- 更新爬虫代码适配新结构
- 临时禁用该数据源

### 问题3：推送重复

**原因**：去重逻辑失效  
**解决**：
```sql
-- 手动清理重复数据
DELETE FROM airdrops a USING airdrops b
WHERE a.id < b.id AND a.title = b.title AND a.platform = b.platform;
```

---

## 📚 参考资料

- [Layer3.xyz API文档](https://layer3.xyz/docs)
- [Galxe GraphQL API](https://docs.galxe.com/)
- [DeFiLlama API](https://defillama.com/docs/api)
- [Twitter API](https://developer.twitter.com/en/docs)
- [Supabase文档](https://supabase.com/docs)

---

## 🎉 下一步计划

- [ ] 添加更多数据源（TaskOn, Zealy等）
- [ ] 实现AI自动筛选（GPT-4集成）
- [ ] 开发管理后台（可视化管理空投）
- [ ] 添加用户反馈机制（点赞/收藏）
- [ ] 实现空投提醒（Telegram Bot）

---

**🚀 祝您的空投系统运行顺利！**

